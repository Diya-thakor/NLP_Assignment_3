{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1**\n",
        "\n",
        "## Question 1\n",
        "\n",
        "## Select the Llama3.2-1B/Gemma mode"
      ],
      "metadata": {
        "id": "4rKkB-ojjcMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"<My Token ID(Hided)>")"
      ],
      "metadata": {
        "_uuid": "a4395d99-1742-4fee-b558-81adddc93f4b",
        "_cell_guid": "67ed3f45-571a-4a80-bbdc-6ebfa4308162",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-11-20T14:14:22.201134Z",
          "iopub.execute_input": "2024-11-20T14:14:22.202062Z",
          "iopub.status.idle": "2024-11-20T14:14:22.418455Z",
          "shell.execute_reply.started": "2024-11-20T14:14:22.202011Z",
          "shell.execute_reply": "2024-11-20T14:14:22.417595Z"
        },
        "trusted": true,
        "id": "B3_QmXGKjcMR",
        "outputId": "d18f5610-8b2c-47f2-ea12-e123c17d7b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "\n",
        "print(whoami())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T14:14:25.134338Z",
          "iopub.execute_input": "2024-11-20T14:14:25.134684Z",
          "iopub.status.idle": "2024-11-20T14:14:25.349259Z",
          "shell.execute_reply.started": "2024-11-20T14:14:25.134654Z",
          "shell.execute_reply": "2024-11-20T14:14:25.348394Z"
        },
        "trusted": true,
        "id": "rAdrZjj2jcMU",
        "outputId": "9dc0964b-4c19-4cd8-a16a-1f16ab9384f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'type': 'user', 'id': '66d47524b005ad82ca135713', 'name': 'Diyat', 'fullname': 'Diya Ashvinsinh Thakor', 'email': 'diyathakor2003@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/f3c067440d2081aa0e305a55495a6b25.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'sst2-model', 'role': 'write', 'createdAt': '2024-11-20T14:14:12.478Z'}}}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "model_classifier_pretrained = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:39:55.754125Z",
          "iopub.execute_input": "2024-11-20T13:39:55.754427Z",
          "iopub.status.idle": "2024-11-20T13:41:02.343598Z",
          "shell.execute_reply.started": "2024-11-20T13:39:55.754402Z",
          "shell.execute_reply": "2024-11-20T13:41:02.342704Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "52275654cd094df28ec3ab0299a917b4",
            "4e7d9ce3c4b44ea594fc26a7fe6488ce",
            "b1386a87c8d649368307ea4be9bf1d89",
            "63058e0101df4781b8eae02799d786ba"
          ]
        },
        "id": "nfTitDu0jcMV",
        "outputId": "c0681516-aac8-45fa-f61a-dda63dbd115b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1386a87c8d649368307ea4be9bf1d89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63058e0101df4781b8eae02799d786ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2**\n",
        "\n",
        "###Calculate the number of parameters of the selected model from the code. Do your calculated parameters match with the parameters reported in the respective papers of the selected model?"
      ],
      "metadata": {
        "id": "ysOYjXVSjcMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of parameters in the model\n",
        "\n",
        "num_params = sum(p.numel() for p in model_classifier_pretrained.parameters())\n",
        "\n",
        "print(f\"Total number of parameters in the model: {num_params/10**9} Billion\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:43:21.868642Z",
          "iopub.execute_input": "2024-11-20T13:43:21.869297Z",
          "iopub.status.idle": "2024-11-20T13:43:21.874542Z",
          "shell.execute_reply.started": "2024-11-20T13:43:21.869262Z",
          "shell.execute_reply": "2024-11-20T13:43:21.873677Z"
        },
        "trusted": true,
        "id": "Q66fsl6RjcMW",
        "outputId": "58f4df6e-de40-43ab-bfa3-24e78da2015a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total number of parameters in the model: 1.235818496 Billion\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prettytable"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:43:29.337184Z",
          "iopub.execute_input": "2024-11-20T13:43:29.337889Z",
          "iopub.status.idle": "2024-11-20T13:43:38.511646Z",
          "shell.execute_reply.started": "2024-11-20T13:43:29.337858Z",
          "shell.execute_reply": "2024-11-20T13:43:38.510740Z"
        },
        "trusted": true,
        "id": "c0MD23KejcMX",
        "outputId": "a4384247-4411-4556-9e0d-622a8b4b9185"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (3.10.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable) (0.2.13)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "\n",
        "    total_params = 0\n",
        "\n",
        "    for name, parameter in model.named_parameters():\n",
        "\n",
        "        if not parameter.requires_grad:\n",
        "\n",
        "            continue\n",
        "\n",
        "        params = parameter.numel()\n",
        "\n",
        "        table.add_row([name, params])\n",
        "\n",
        "        total_params += params\n",
        "\n",
        "    print(table)\n",
        "\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "\n",
        "    return total_params\n",
        "\n",
        "\n",
        "\n",
        "# Count and display the parameters of the model\n",
        "\n",
        "count_parameters(model_classifier_pretrained)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:43:45.088170Z",
          "iopub.execute_input": "2024-11-20T13:43:45.088540Z",
          "iopub.status.idle": "2024-11-20T13:43:45.131894Z",
          "shell.execute_reply.started": "2024-11-20T13:43:45.088505Z",
          "shell.execute_reply": "2024-11-20T13:43:45.131091Z"
        },
        "trusted": true,
        "id": "6TGBuyMHjcMX",
        "outputId": "4dd31aef-7420-4905-a143-3cfb78187a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "+-------------------------------------------------+------------+\n|                     Modules                     | Parameters |\n+-------------------------------------------------+------------+\n|            model.embed_tokens.weight            | 262668288  |\n|      model.layers.0.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.0.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.0.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.0.mlp.up_proj.weight        |  16777216  |\n|       model.layers.0.mlp.down_proj.weight       |  16777216  |\n|      model.layers.0.input_layernorm.weight      |    2048    |\n|  model.layers.0.post_attention_layernorm.weight |    2048    |\n|      model.layers.1.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.1.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.1.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.1.mlp.up_proj.weight        |  16777216  |\n|       model.layers.1.mlp.down_proj.weight       |  16777216  |\n|      model.layers.1.input_layernorm.weight      |    2048    |\n|  model.layers.1.post_attention_layernorm.weight |    2048    |\n|      model.layers.2.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.2.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.2.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.2.mlp.up_proj.weight        |  16777216  |\n|       model.layers.2.mlp.down_proj.weight       |  16777216  |\n|      model.layers.2.input_layernorm.weight      |    2048    |\n|  model.layers.2.post_attention_layernorm.weight |    2048    |\n|      model.layers.3.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.3.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.3.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.3.mlp.up_proj.weight        |  16777216  |\n|       model.layers.3.mlp.down_proj.weight       |  16777216  |\n|      model.layers.3.input_layernorm.weight      |    2048    |\n|  model.layers.3.post_attention_layernorm.weight |    2048    |\n|      model.layers.4.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.4.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.4.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.4.mlp.up_proj.weight        |  16777216  |\n|       model.layers.4.mlp.down_proj.weight       |  16777216  |\n|      model.layers.4.input_layernorm.weight      |    2048    |\n|  model.layers.4.post_attention_layernorm.weight |    2048    |\n|      model.layers.5.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.5.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.5.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.5.mlp.up_proj.weight        |  16777216  |\n|       model.layers.5.mlp.down_proj.weight       |  16777216  |\n|      model.layers.5.input_layernorm.weight      |    2048    |\n|  model.layers.5.post_attention_layernorm.weight |    2048    |\n|      model.layers.6.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.6.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.6.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.6.mlp.up_proj.weight        |  16777216  |\n|       model.layers.6.mlp.down_proj.weight       |  16777216  |\n|      model.layers.6.input_layernorm.weight      |    2048    |\n|  model.layers.6.post_attention_layernorm.weight |    2048    |\n|      model.layers.7.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.7.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.7.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.7.mlp.up_proj.weight        |  16777216  |\n|       model.layers.7.mlp.down_proj.weight       |  16777216  |\n|      model.layers.7.input_layernorm.weight      |    2048    |\n|  model.layers.7.post_attention_layernorm.weight |    2048    |\n|      model.layers.8.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.8.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.8.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.8.mlp.up_proj.weight        |  16777216  |\n|       model.layers.8.mlp.down_proj.weight       |  16777216  |\n|      model.layers.8.input_layernorm.weight      |    2048    |\n|  model.layers.8.post_attention_layernorm.weight |    2048    |\n|      model.layers.9.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.9.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.9.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.9.mlp.up_proj.weight        |  16777216  |\n|       model.layers.9.mlp.down_proj.weight       |  16777216  |\n|      model.layers.9.input_layernorm.weight      |    2048    |\n|  model.layers.9.post_attention_layernorm.weight |    2048    |\n|     model.layers.10.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.10.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.10.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.10.mlp.up_proj.weight       |  16777216  |\n|       model.layers.10.mlp.down_proj.weight      |  16777216  |\n|      model.layers.10.input_layernorm.weight     |    2048    |\n| model.layers.10.post_attention_layernorm.weight |    2048    |\n|     model.layers.11.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.11.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.11.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.11.mlp.up_proj.weight       |  16777216  |\n|       model.layers.11.mlp.down_proj.weight      |  16777216  |\n|      model.layers.11.input_layernorm.weight     |    2048    |\n| model.layers.11.post_attention_layernorm.weight |    2048    |\n|     model.layers.12.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.12.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.12.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.12.mlp.up_proj.weight       |  16777216  |\n|       model.layers.12.mlp.down_proj.weight      |  16777216  |\n|      model.layers.12.input_layernorm.weight     |    2048    |\n| model.layers.12.post_attention_layernorm.weight |    2048    |\n|     model.layers.13.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.13.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.13.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.13.mlp.up_proj.weight       |  16777216  |\n|       model.layers.13.mlp.down_proj.weight      |  16777216  |\n|      model.layers.13.input_layernorm.weight     |    2048    |\n| model.layers.13.post_attention_layernorm.weight |    2048    |\n|     model.layers.14.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.14.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.14.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.14.mlp.up_proj.weight       |  16777216  |\n|       model.layers.14.mlp.down_proj.weight      |  16777216  |\n|      model.layers.14.input_layernorm.weight     |    2048    |\n| model.layers.14.post_attention_layernorm.weight |    2048    |\n|     model.layers.15.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.15.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.15.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.15.mlp.up_proj.weight       |  16777216  |\n|       model.layers.15.mlp.down_proj.weight      |  16777216  |\n|      model.layers.15.input_layernorm.weight     |    2048    |\n| model.layers.15.post_attention_layernorm.weight |    2048    |\n|                model.norm.weight                |    2048    |\n|                   score.weight                  |    4096    |\n+-------------------------------------------------+------------+\nTotal Trainable Params: 1235818496\n",
          "output_type": "stream"
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "1235818496"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Answer:**\n",
        "\n",
        "\n",
        "\n",
        "We selected the model-\"Llama-3.2-1B\".\n",
        "\n",
        "The number of parameters of the Llama-3.2-1B model from the code is  (1.23 B).\n",
        "\n",
        "The calculated parameters match with the parameters reported in the below official documents of this 'Llama-3.2-1B' model.\n",
        "\n",
        "\n",
        "\n",
        "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"
      ],
      "metadata": {
        "id": "lDc4FCEzkGjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 3:**\n",
        "\n",
        "##(b)Fine-tune the pre-trained model on the **CLASSIFICATION: SST2**"
      ],
      "metadata": {
        "id": "wh4DFjrObMh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [a] Classification: SST-2s:"
      ],
      "metadata": {
        "id": "y9jIuppQjcMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:43:57.599762Z",
          "iopub.execute_input": "2024-11-20T13:43:57.600574Z",
          "iopub.status.idle": "2024-11-20T13:44:05.854617Z",
          "shell.execute_reply.started": "2024-11-20T13:43:57.600523Z",
          "shell.execute_reply": "2024-11-20T13:44:05.853552Z"
        },
        "trusted": true,
        "id": "RKqKlX7IjcMd",
        "outputId": "b000d3de-3707-43db-a509-c2a492d232b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Load the SST-2 dataset\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:44:11.967923Z",
          "iopub.execute_input": "2024-11-20T13:44:11.968805Z",
          "iopub.status.idle": "2024-11-20T13:44:24.426386Z",
          "shell.execute_reply.started": "2024-11-20T13:44:11.968766Z",
          "shell.execute_reply": "2024-11-20T13:44:24.425517Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "f6f4041c74b9417a8ea2580af4afa9ad",
            "c26fcdab11cf4c9ca027fb3c5aa69695",
            "38c9ce3565714126abd129bae61d7288",
            "d611d1123f3041339cc063b07ff49701",
            "30a4547ec9fe4412a075658bc43fa5e6",
            "b91a8b7bf4eb40abb5f5cf8899ab970a",
            "0d1de24d171b48bd94e9f406aa192a71",
            "a9e9211224d64ceea221bcf239c977c6",
            "9c5bf7569e494d519ab051dacf13cd13",
            "b8da4597b884425990ee55bd1a05bec4",
            "923047b45e3a4e5082080bbe83283519",
            "7bc62fdcddcf4b49bb3a7dfbe4d858a4",
            "2d0b0c4106b6472393e47b9bb83d4960",
            "d1eea151b06a4dc28e7e147ccbefc01b"
          ]
        },
        "id": "EGXXU_nVjcMf",
        "outputId": "1cd0ee77-526b-44a2-e07e-556513e0dfef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9e9211224d64ceea221bcf239c977c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c5bf7569e494d519ab051dacf13cd13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8da4597b884425990ee55bd1a05bec4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "923047b45e3a4e5082080bbe83283519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bc62fdcddcf4b49bb3a7dfbe4d858a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0b0c4106b6472393e47b9bb83d4960"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1eea151b06a4dc28e7e147ccbefc01b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 1821\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the HuggingFace dataset to a Pandas DataFrame for compatibility with train_test_split\n",
        "\n",
        "df_mainTrain_sst = dataset['train'].to_pandas()\n",
        "\n",
        "df_mainTest_sst = dataset['test'].to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "print(\"The Shape of training data of sst2:-\", df_mainTrain_sst.shape)\n",
        "\n",
        "print(\"The training dataset of sst2:\", df_mainTrain_sst.head())\n",
        "\n",
        "print(\"The Shape of test data of sst2:-\", df_mainTest_sst.shape)\n",
        "\n",
        "print(\"The training dataset of sst2:\", df_mainTest_sst.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Split into train and test (80-20 split)\n",
        "\n",
        "train_df_sst, test_df_sst = train_test_split(df_mainTrain_sst, test_size=0.2, random_state=1, stratify=df_mainTrain_sst['label'])\n",
        "\n",
        "\n",
        "\n",
        "# Convert back to HuggingFace dataset format\n",
        "\n",
        "# from datasets import Dataset\n",
        "\n",
        "# train_dataset_sst = Dataset.from_pandas(train_df_sst)\n",
        "\n",
        "# test_dataset_sst = Dataset.from_pandas(test_df_sst)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:44:27.718368Z",
          "iopub.execute_input": "2024-11-20T13:44:27.718713Z",
          "iopub.status.idle": "2024-11-20T13:44:27.804453Z",
          "shell.execute_reply.started": "2024-11-20T13:44:27.718681Z",
          "shell.execute_reply": "2024-11-20T13:44:27.803593Z"
        },
        "trusted": true,
        "id": "79H9p9c3jcMg",
        "outputId": "c5ff89ea-cfaf-48e0-b588-ca2676f77523"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The Shape of training data of sst2:- (67349, 3)\nThe training dataset of sst2:    idx                                           sentence  label\n0    0       hide new secretions from the parental units       0\n1    1               contains no wit , only labored gags       0\n2    2  that loves its characters and communicates som...      1\n3    3  remains utterly satisfied to remain the same t...      0\n4    4  on the worst revenge-of-the-nerds clichés the ...      0\nThe Shape of test data of sst2:- (1821, 3)\nThe training dataset of sst2:    idx                                           sentence  label\n0    0             uneasy mishmash of styles and genres .     -1\n1    1  this film 's relationship to actual tension is...     -1\n2    2  by the end of no such thing the audience , lik...     -1\n3    3  director rob marshall went out gunning to make...     -1\n4    4  lathan and diggs have considerable personal ch...     -1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Original dataset with train test splits:\")\n",
        "\n",
        "\n",
        "\n",
        "label_counts_sst_trainMain= df_mainTrain_sst['label'].value_counts()\n",
        "\n",
        "label_counts_sst_testMain= df_mainTest_sst['label'].value_counts() #something\n",
        "\n",
        "print(label_counts_sst_trainMain)\n",
        "\n",
        "print(label_counts_sst_testMain)\n",
        "\n",
        "\n",
        "\n",
        "print(\"**************************************\")\n",
        "\n",
        "print(\"Train-test splits from Train data only:\")\n",
        "\n",
        "\n",
        "\n",
        "label_counts_sst_train= train_df_sst['label'].value_counts()\n",
        "\n",
        "label_counts_sst_test= test_df_sst['label'].value_counts()\n",
        "\n",
        "print(label_counts_sst_train)\n",
        "\n",
        "print(label_counts_sst_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:44:34.059097Z",
          "iopub.execute_input": "2024-11-20T13:44:34.059442Z",
          "iopub.status.idle": "2024-11-20T13:44:34.076889Z",
          "shell.execute_reply.started": "2024-11-20T13:44:34.059413Z",
          "shell.execute_reply": "2024-11-20T13:44:34.076107Z"
        },
        "trusted": true,
        "id": "3-q6BkHAjcMh",
        "outputId": "0f0bf10d-4dfe-405e-ac30-774c7269e3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The Original dataset with train test splits:\nlabel\n1    37569\n0    29780\nName: count, dtype: int64\nlabel\n-1    1821\nName: count, dtype: int64\n**************************************\nTrain-test splits from Train data only:\nlabel\n1    30055\n0    23824\nName: count, dtype: int64\nlabel\n1    7514\n0    5956\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#printing data\n",
        "\n",
        "print(\"The training data:-\")\n",
        "\n",
        "print(train_df_sst.head())\n",
        "\n",
        "print(train_df_sst.shape)\n",
        "\n",
        "print(\"\\n***********************************************\")\n",
        "\n",
        "print(\"The test data:-\")\n",
        "\n",
        "print(test_df_sst.head())\n",
        "\n",
        "print(test_df_sst.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:44:38.270778Z",
          "iopub.execute_input": "2024-11-20T13:44:38.271141Z",
          "iopub.status.idle": "2024-11-20T13:44:38.279234Z",
          "shell.execute_reply.started": "2024-11-20T13:44:38.271109Z",
          "shell.execute_reply": "2024-11-20T13:44:38.278367Z"
        },
        "trusted": true,
        "id": "tGFw00QEjcMi",
        "outputId": "87f7a6ec-f70a-45dc-9fbf-882e469247d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The training data:-\n         idx                                           sentence  label\n18071  18071  poses for itself that one can forgive the film...      1\n22251  22251  it 's tough to tell which is in more abundant ...      0\n13938  13938                                   the first movie       1\n58240  58240                                            charms       1\n39721  39721  the narrator and the other characters try to c...      0\n(53879, 3)\n\n***********************************************\nThe test data:-\n         idx                                          sentence  label\n5536    5536  carvey 's considerable talents are wasted in it       0\n31437  31437                   wrap the proceedings up neatly       1\n44037  44037                            irritates and saddens       0\n27492  27492                           a magnetic performance       1\n35062  35062                       as plain and pedestrian as       0\n(13470, 3)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "# Convert Pandas DataFrames back to Hugging Face Dataset format\n",
        "\n",
        "train_dataset_sst = Dataset.from_pandas(train_df_sst)\n",
        "\n",
        "test_dataset_sst = Dataset.from_pandas(test_df_sst)\n",
        "\n",
        "\n",
        "\n",
        "# Define the tokenize function\n",
        "\n",
        "def tokenize_function(examples):\n",
        "\n",
        "    return tokenizer(examples['sentence'], padding=True, truncation=True)\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the datasets\n",
        "\n",
        "train_dataset_sst = train_dataset_sst.map(tokenize_function, batched=True)\n",
        "\n",
        "test_dataset_sst = test_dataset_sst.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:44:42.107261Z",
          "iopub.execute_input": "2024-11-20T13:44:42.108168Z",
          "iopub.status.idle": "2024-11-20T13:44:49.035751Z",
          "shell.execute_reply.started": "2024-11-20T13:44:42.108119Z",
          "shell.execute_reply": "2024-11-20T13:44:49.034818Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "ae7f8a19e1da4994b9de1190b6dc9f19",
            "33dbf0a2a0f945c99c8cb83070a81e3e",
            "8c8fce5ffc174060b8a22e5cd6157854",
            "943cbd95d1d14a4bb085af4e8bc8691c",
            "f042e05b19cb4b3fb784555ea9d90318",
            "0f8525a1deff461d8cbcd697246d4519",
            "31d4c8f3adad4e349cbe8904cd1dc3ed",
            "5fdaff1b88db4ec59f353fba7102bfcc",
            "d261be20213443338a0a05768fcda210",
            "20a58ddb96864d449c3d09e0fb7cad6e"
          ]
        },
        "id": "3MDaHoyJjcMj",
        "outputId": "a095bed6-e000-4e43-b628-9c586bb395b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8525a1deff461d8cbcd697246d4519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31d4c8f3adad4e349cbe8904cd1dc3ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fdaff1b88db4ec59f353fba7102bfcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/53879 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d261be20213443338a0a05768fcda210"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/13470 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a58ddb96864d449c3d09e0fb7cad6e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:45:40.415414Z",
          "iopub.execute_input": "2024-11-20T13:45:40.415760Z",
          "iopub.status.idle": "2024-11-20T13:45:52.701359Z",
          "shell.execute_reply.started": "2024-11-20T13:45:40.415728Z",
          "shell.execute_reply": "2024-11-20T13:45:52.700424Z"
        },
        "trusted": true,
        "id": "ZkagHHKTjcMj",
        "outputId": "bfb19da7-4e22-4e52-dcb4-64d796dd8a28"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AutoTokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:45:59.016352Z",
          "iopub.execute_input": "2024-11-20T13:45:59.017065Z",
          "iopub.status.idle": "2024-11-20T13:46:11.297382Z",
          "shell.execute_reply.started": "2024-11-20T13:45:59.017005Z",
          "shell.execute_reply": "2024-11-20T13:46:11.296439Z"
        },
        "trusted": true,
        "id": "ySRSYI-5jcMk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "6d361caab36f2b46b4866bf707dbe54059d338c8\n"
      ],
      "metadata": {
        "id": "GftxMoUFjcMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import bitsandbytes # Import the library explicitly after installation\n",
        "\n",
        "train_dataset_sst = train_dataset_sst.select(range(1000))\n",
        "\n",
        "test_dataset_sst = test_dataset_sst.select(range(200))\n",
        "\n",
        "\n",
        "\n",
        "model_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    num_labels=2,\n",
        "\n",
        "    # load_in_8bit=True  # Enables 8-bit quantization\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "\n",
        "model_classifier.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "\n",
        "# Freeze lower layers to save memory and computation\n",
        "\n",
        "for param in model_classifier.base_model.parameters():\n",
        "\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "# Define a data collator with padding for batching\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/kaggle/working/results\",  # Save to Kaggle's working directory\n",
        "\n",
        "    evaluation_strategy=\"steps\",  # Evaluate periodically instead of every epoch\n",
        "\n",
        "    save_strategy=\"steps\",        # Save checkpoints periodically\n",
        "\n",
        "    save_steps=500,               # Save every 500 steps\n",
        "\n",
        "    save_total_limit=1,           # Keep only the latest checkpoint\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "\n",
        "    per_device_eval_batch_size=1,\n",
        "\n",
        "    gradient_accumulation_steps=4,  # Simulate larger batch size\n",
        "\n",
        "    num_train_epochs=10,  # Use fewer epochs for testing\n",
        "\n",
        "    seed=1,\n",
        "\n",
        "    fp16=False,  # Mixed precision training\n",
        "\n",
        "    logging_dir=\"/kaggle/working/logs\",  # Change logging directory to avoid /tmp issues\n",
        "\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "\n",
        "    log_level=\"error\",  # Reduce logging verbosity\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_classifier = model_classifier.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Setup the Trainer\n",
        "\n",
        "model_classifier_finetuned = Trainer(\n",
        "\n",
        "    model=model_classifier,\n",
        "\n",
        "    args=training_args,\n",
        "\n",
        "    train_dataset=train_dataset_sst,\n",
        "\n",
        "    eval_dataset=test_dataset_sst,\n",
        "\n",
        "    data_collator=data_collator,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# # Monitor GPU memory\n",
        "\n",
        "# print(\"Before training:\")\n",
        "\n",
        "# !nvidia-smi\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model_classifier_finetuned.train()\n",
        "\n",
        "\n",
        "\n",
        "# # Monitor GPU memory post-training\n",
        "\n",
        "# print(\"After training:\")\n",
        "\n",
        "# !nvidia-smi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:46:15.550820Z",
          "iopub.execute_input": "2024-11-20T13:46:15.551512Z",
          "iopub.status.idle": "2024-11-20T13:55:17.464156Z",
          "shell.execute_reply.started": "2024-11-20T13:46:15.551477Z",
          "shell.execute_reply": "2024-11-20T13:55:17.463421Z"
        },
        "trusted": true,
        "id": "iVSclhoMjcMm",
        "outputId": "73a79231-2b7e-4b15-e303-cd7fe4bcd0d5",
        "colab": {
          "referenced_widgets": [
            "1886fbf7e3f542dc95d328e82c4e08c8"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111368656666577, max=1.0)…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1886fbf7e3f542dc95d328e82c4e08c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241120_134639-m5t14bp6</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface/runs/m5t14bp6' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface/runs/m5t14bp6' target=\"_blank\">https://wandb.ai/sonishivansh210-iit-gandhinagar/huggingface/runs/m5t14bp6</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 08:33, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.796400</td>\n      <td>0.797361</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.754400</td>\n      <td>0.759815</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.723200</td>\n      <td>0.760601</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.728700</td>\n      <td>0.763997</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.730800</td>\n      <td>0.741848</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.697800</td>\n      <td>0.742757</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.719600</td>\n      <td>0.731416</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.703600</td>\n      <td>0.731402</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.684400</td>\n      <td>0.726821</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.701900</td>\n      <td>0.721235</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.698500</td>\n      <td>0.720082</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.680100</td>\n      <td>0.724849</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.690700</td>\n      <td>0.716751</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.680800</td>\n      <td>0.708793</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.668000</td>\n      <td>0.732707</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.675400</td>\n      <td>0.708816</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.664300</td>\n      <td>0.704469</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.665600</td>\n      <td>0.704456</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.648800</td>\n      <td>0.724003</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.695300</td>\n      <td>0.700858</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.666300</td>\n      <td>0.708543</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.653400</td>\n      <td>0.702104</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.675200</td>\n      <td>0.702379</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.644000</td>\n      <td>0.701244</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.677300</td>\n      <td>0.699682</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=2500, training_loss=0.6929761993408203, metrics={'train_runtime': 535.5184, 'train_samples_per_second': 18.673, 'train_steps_per_second': 4.668, 'total_flos': 3561729761280000.0, 'train_loss': 0.6929761993408203, 'epoch': 10.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier_finetuned.save_model(\"/kaggle/working/results\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T13:57:57.431984Z",
          "iopub.execute_input": "2024-11-20T13:57:57.432421Z",
          "iopub.status.idle": "2024-11-20T13:58:13.727340Z",
          "shell.execute_reply.started": "2024-11-20T13:57:57.432387Z",
          "shell.execute_reply": "2024-11-20T13:58:13.726123Z"
        },
        "trusted": true,
        "id": "u63eAg5wjcMn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming these are the variables you might want to delete\n",
        "\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model_classifier_finetuned.model.to('cpu')\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-20T14:11:19.960522Z",
          "iopub.execute_input": "2024-11-20T14:11:19.960868Z",
          "iopub.status.idle": "2024-11-20T14:11:24.581394Z",
          "shell.execute_reply.started": "2024-11-20T14:11:19.960838Z",
          "shell.execute_reply": "2024-11-20T14:11:24.580462Z"
        },
        "trusted": true,
        "id": "QLTV2s4xjcMo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4:**\n",
        "\n",
        "\n",
        "\n",
        "##Calculate the scores for the following metrics on the test splits for the pre-trained (zero-shot) and fine-tuned models.\n",
        "\n",
        "\n",
        "\n",
        "###**Classification: Accuracy, Precision, Recall, F1**"
      ],
      "metadata": {
        "id": "glEbcsm5btHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Load the pre-trained model\n",
        "model_pretrained = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model_pretrained.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/results\")\n",
        "model_finetuned.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Make predictions\n",
        "def get_predictions(model, dataset):\n",
        "    trainer = Trainer(model=model)\n",
        "    predictions = trainer.predict(dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    return preds, predictions.label_ids\n",
        "\n",
        "pretrained_preds, labels = get_predictions(model_pretrained, test_dataset_sst)\n",
        "finetuned_preds, labels = get_predictions(model_finetuned, test_dataset_sst)\n",
        "\n",
        "# Calculate Metrics\n",
        "def calculate_metrics(predictions, labels):\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "pretrained_metrics = calculate_metrics(pretrained_preds, labels)\n",
        "finetuned_metrics = calculate_metrics(finetuned_preds, labels)\n",
        "\n",
        "# Print Metrics\n",
        "print(\"Pre-trained Model Metrics:\")\n",
        "print(f\"Accuracy: {pretrained_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {pretrained_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {pretrained_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {pretrained_metrics[3]:.4f}\")\n",
        "\n",
        "print(\"\\nFine-tuned Model Metrics:\")\n",
        "print(f\"Accuracy: {finetuned_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {finetuned_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {finetuned_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {finetuned_metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T14:09:12.939503Z",
          "iopub.execute_input": "2024-11-20T14:09:12.940246Z",
          "iopub.status.idle": "2024-11-20T14:09:25.433427Z",
          "shell.execute_reply.started": "2024-11-20T14:09:12.940212Z",
          "shell.execute_reply": "2024-11-20T14:09:25.432529Z"
        },
        "id": "0J4FHgRg9TKN",
        "outputId": "b5ec86fe-d328-411b-dd87-56ea43349bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Pre-trained Model Metrics:\nAccuracy: 0.4850\nPrecision: 0.4762\nRecall: 0.0980\nF1-Score: 0.1626\n\nFine-tuned Model Metrics:\nAccuracy: 0.5100\nPrecision: 0.5108\nRecall: 0.9314\nF1-Score: 0.6597\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5**\n",
        "\n",
        "\n",
        "\n",
        "##Calculate the number of parameters in the model after fine-tuning. Does it remain the same as the pre-trained model?"
      ],
      "metadata": {
        "id": "P1qmzyW5cZZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "pretrained_params = count_parameters(model_classifier.base_model)\n",
        "print(f\"Number of parameters in pre-trained model: {pretrained_params}\")\n",
        "\n",
        "\n",
        "finetuned_params = count_parameters(model_classifier_finetuned.model)\n",
        "print(f\"Number of parameters in fine-tuned model: {finetuned_params}\")\n"
      ],
      "metadata": {
        "id": "7Y34Zmg5mlqu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T14:12:11.099251Z",
          "iopub.execute_input": "2024-11-20T14:12:11.099639Z",
          "iopub.status.idle": "2024-11-20T14:12:11.111229Z",
          "shell.execute_reply.started": "2024-11-20T14:12:11.099606Z",
          "shell.execute_reply": "2024-11-20T14:12:11.110370Z"
        },
        "outputId": "4c205168-f865-4216-8fee-b0402bca39cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of parameters in pre-trained model: 1235814400\nNumber of parameters in fine-tuned model: 1235818496\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6:**\n",
        "\n",
        "\n",
        "\n",
        "##Push the fine-tuned model to 🤗."
      ],
      "metadata": {
        "id": "MZgjHUFxcecL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "#model_qa_finetuned.save_pretrained(\"path_to_save_model\")\n",
        "#tokenizer.save_pretrained(\"path_to_save_model\")\n",
        "\n",
        "# Create a repo on Hugging Face (if not already created)\n",
        "create_repo(\"Diyat/sst2-fineTune\", exist_ok=True)\n",
        "\n",
        "# Upload to Hugging Face\n",
        "upload_folder(\n",
        "    folder_path=\"/kaggle/working/results\",\n",
        "    repo_id=\"Diyat/sst2-fineTune\",\n",
        "    commit_message=\"Fine-tuned model on SST2 dataset\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1a0BCmbFmnyY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T14:14:38.365206Z",
          "iopub.execute_input": "2024-11-20T14:14:38.365652Z",
          "iopub.status.idle": "2024-11-20T14:18:21.122595Z",
          "shell.execute_reply.started": "2024-11-20T14:14:38.365621Z",
          "shell.execute_reply": "2024-11-20T14:18:21.121715Z"
        },
        "outputId": "577c3060-55d5-4a2c-dcc0-9b79ad616a91",
        "colab": {
          "referenced_widgets": [
            "92854f2823874e77ad8f7c7657dda53b",
            "30a031e2544c49f0ba64551871fb894f",
            "2c9cae0866034007b2fa1c3207391b19",
            "2194a166fa454c3a9444cf9f30760c48",
            "57e9093a6d9a400cbd28621949fbbdb8",
            "3980f705b816406a99de4eea5dc7ed03",
            "3e48d7856e2f41b2aa5e913702733e04",
            "e094e9f4151b44e6aa5f8e12aee3414d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92854f2823874e77ad8f7c7657dda53b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30a031e2544c49f0ba64551871fb894f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c9cae0866034007b2fa1c3207391b19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "optimizer.pt:   0%|          | 0.00/34.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2194a166fa454c3a9444cf9f30760c48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57e9093a6d9a400cbd28621949fbbdb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3980f705b816406a99de4eea5dc7ed03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e48d7856e2f41b2aa5e913702733e04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e094e9f4151b44e6aa5f8e12aee3414d"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Diyat/sst2-fineTune/commit/b715a09e3782226baee23689dda949ed08e4d35b', commit_message='Fine-tuned model on SST2 dataset', commit_description='', oid='b715a09e3782226baee23689dda949ed08e4d35b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Diyat/sst2-fineTune', endpoint='https://huggingface.co', repo_type='model', repo_id='Diyat/sst2-fineTune'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ]
}
