{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 3:**\n",
        "##(b)Fine-tune the pre-trained model on the **Question-Answering: SQuAD**"
      ],
      "metadata": {
        "id": "wh4DFjrObMh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"<My Token ID>\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-19T17:18:53.052778Z",
          "iopub.execute_input": "2024-11-19T17:18:53.053022Z",
          "iopub.status.idle": "2024-11-19T17:18:53.546325Z",
          "shell.execute_reply.started": "2024-11-19T17:18:53.052991Z",
          "shell.execute_reply": "2024-11-19T17:18:53.545454Z"
        },
        "trusted": true,
        "id": "LGU5pi-6aXtc",
        "outputId": "592efa8c-b461-4b19-be39-6007514055d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:13.645041Z",
          "iopub.execute_input": "2024-11-19T17:19:13.645313Z",
          "iopub.status.idle": "2024-11-19T17:19:16.726844Z",
          "shell.execute_reply.started": "2024-11-19T17:19:13.645278Z",
          "shell.execute_reply": "2024-11-19T17:19:16.726175Z"
        },
        "trusted": true,
        "id": "1BT-Pl8raXtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Squad Dataset"
      ],
      "metadata": {
        "id": "2vAMSkPndeBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the  dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "print(squad_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:18:53.547789Z",
          "iopub.execute_input": "2024-11-19T17:18:53.548043Z",
          "iopub.status.idle": "2024-11-19T17:19:09.520859Z",
          "shell.execute_reply.started": "2024-11-19T17:18:53.548017Z",
          "shell.execute_reply": "2024-11-19T17:19:09.519875Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6c1b25b8d86c4d248b1f95345d6d7680",
            "ded69a660df749439fa526d2705bee92",
            "fe71c2665baa41b8b980cf6e25a7ecc4",
            "51038d799721446b909187bf5255932c",
            "6292bc463e3742f2920bb927d21749ff"
          ]
        },
        "id": "0mjl4ykraXti",
        "outputId": "b7cd9c9d-139b-432f-a1b4-9f1f98020605"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c1b25b8d86c4d248b1f95345d6d7680"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ded69a660df749439fa526d2705bee92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe71c2665baa41b8b980cf6e25a7ecc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51038d799721446b909187bf5255932c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6292bc463e3742f2920bb927d21749ff"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 130319\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 11873\n    })\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = squad_dataset['train']\n",
        "x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:09.522119Z",
          "iopub.execute_input": "2024-11-19T17:19:09.523149Z",
          "iopub.status.idle": "2024-11-19T17:19:09.528994Z",
          "shell.execute_reply.started": "2024-11-19T17:19:09.523088Z",
          "shell.execute_reply": "2024-11-19T17:19:09.528175Z"
        },
        "trusted": true,
        "id": "lDV9-n11aXtj",
        "outputId": "ae4779fd-c51b-44fc-ff65-a03b3ae6afb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 130319\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_squad, test_squad = train_test_split(squad_dataset['train'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:09.530681Z",
          "iopub.execute_input": "2024-11-19T17:19:09.530929Z",
          "iopub.status.idle": "2024-11-19T17:19:13.642547Z",
          "shell.execute_reply.started": "2024-11-19T17:19:09.530905Z",
          "shell.execute_reply": "2024-11-19T17:19:13.641800Z"
        },
        "trusted": true,
        "id": "shUJQIacaXtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 5 samples from the training dataset\n",
        "first_5_samples = squad_dataset['train'].select(range(5))\n",
        "\n",
        "first_5_samples_list = first_5_samples[:]\n",
        "print(first_5_samples_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:16.728010Z",
          "iopub.execute_input": "2024-11-19T17:19:16.728380Z",
          "iopub.status.idle": "2024-11-19T17:19:16.737610Z",
          "shell.execute_reply.started": "2024-11-19T17:19:16.728343Z",
          "shell.execute_reply": "2024-11-19T17:19:16.736698Z"
        },
        "trusted": true,
        "id": "RqbFPPtmaXtl",
        "outputId": "809a97cd-e0ea-4dfe-9469-b5c3ff653597"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'id': ['56be85543aeaaa14008c9063', '56be85543aeaaa14008c9065', '56be85543aeaaa14008c9066', '56bf6b0f3aeaaa14008c9601', '56bf6b0f3aeaaa14008c9602'], 'title': ['Beyoncé', 'Beyoncé', 'Beyoncé', 'Beyoncé', 'Beyoncé'], 'context': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'], 'question': ['When did Beyonce start becoming popular?', 'What areas did Beyonce compete in when she was growing up?', \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'In what city and state did Beyonce  grow up? ', 'In which decade did Beyonce become famous?'], 'answers': [{'text': ['in the late 1990s'], 'answer_start': [269]}, {'text': ['singing and dancing'], 'answer_start': [207]}, {'text': ['2003'], 'answer_start': [526]}, {'text': ['Houston, Texas'], 'answer_start': [166]}, {'text': ['late 1990s'], 'answer_start': [276]}]}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_squad(examples):\n",
        "    # Tokenize the context and question pair\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples['question'], examples['context'], truncation=True, padding='max_length', max_length=512\n",
        "    )\n",
        "\n",
        "    # Initialize lists to store the positions\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i in range(len(examples['answers'])):\n",
        "        answer = examples['answers'][i]\n",
        "\n",
        "        # Check if the answer is empty\n",
        "        if not answer['answer_start'] or not answer['text']:\n",
        "            # If no answer exists, append dummy values (e.g., 0)\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "            continue\n",
        "\n",
        "        answer_start = answer['answer_start'][0]  # Assuming there's only one answer per question\n",
        "        answer_text = answer['text'][0]\n",
        "\n",
        "        # Find the position of the answer in the tokenized context\n",
        "        start_position = tokenized_examples.char_to_token(answer_start)\n",
        "        end_position = tokenized_examples.char_to_token(answer_start + len(answer_text))\n",
        "\n",
        "        # If we couldn't find the token for the answer, mark the positions as 0\n",
        "        if start_position is None or end_position is None:\n",
        "            start_position = end_position = 0\n",
        "\n",
        "        start_positions.append(start_position)\n",
        "        end_positions.append(end_position)\n",
        "\n",
        "    # Add the positions to the tokenized examples\n",
        "    tokenized_examples['start_positions'] = start_positions\n",
        "    tokenized_examples['end_positions'] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "train_encodings = squad_dataset['train'].shuffle(seed=1).select(range(1000)).map(preprocess_squad, batched=True)\n",
        "test_encodings = squad_dataset['validation'].shuffle(seed=1).select(range(200)).map(preprocess_squad, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:16.738699Z",
          "iopub.execute_input": "2024-11-19T17:19:16.738964Z",
          "iopub.status.idle": "2024-11-19T17:19:20.451969Z",
          "shell.execute_reply.started": "2024-11-19T17:19:16.738941Z",
          "shell.execute_reply": "2024-11-19T17:19:20.451207Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "eada12f1ff6740cd8edb860d34d27e56",
            "6ddd1219a6c04c1cab38360628eca988",
            "7ba66df75ad042618dbb50761d14c365",
            "0f82a8b6b00a403789d2f9bfa2011201",
            "c5090446dedc4f1ab6d84f673936feb7",
            "e07dc6006f024d42966c2807bb82845a"
          ]
        },
        "id": "qRshsSxwaXtm",
        "outputId": "b9757d74-1bf5-4bee-80d1-328b056db652"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eada12f1ff6740cd8edb860d34d27e56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ddd1219a6c04c1cab38360628eca988"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ba66df75ad042618dbb50761d14c365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f82a8b6b00a403789d2f9bfa2011201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5090446dedc4f1ab6d84f673936feb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/200 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e07dc6006f024d42966c2807bb82845a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:20.453081Z",
          "iopub.execute_input": "2024-11-19T17:19:20.453437Z",
          "iopub.status.idle": "2024-11-19T17:19:20.468266Z",
          "shell.execute_reply.started": "2024-11-19T17:19:20.453410Z",
          "shell.execute_reply": "2024-11-19T17:19:20.467362Z"
        },
        "trusted": true,
        "id": "Dd9ggl-maXtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:19:20.469254Z",
          "iopub.execute_input": "2024-11-19T17:19:20.469513Z",
          "iopub.status.idle": "2024-11-19T17:20:32.596159Z",
          "shell.execute_reply.started": "2024-11-19T17:19:20.469489Z",
          "shell.execute_reply": "2024-11-19T17:20:32.595368Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "1c99d4124c5f437d92528e170179049a",
            "285fb2b243744deabbcc5d4056bd7750"
          ]
        },
        "id": "rvpSYPVHaXtn",
        "outputId": "a8e3567e-7c7d-441c-9113-be43385dcda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c99d4124c5f437d92528e170179049a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "285fb2b243744deabbcc5d4056bd7750"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AutoTokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:20:32.598883Z",
          "iopub.execute_input": "2024-11-19T17:20:32.599762Z",
          "iopub.status.idle": "2024-11-19T17:20:44.433269Z",
          "shell.execute_reply.started": "2024-11-19T17:20:32.599720Z",
          "shell.execute_reply": "2024-11-19T17:20:44.432502Z"
        },
        "trusted": true,
        "id": "2OD489itaXtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa.gradient_checkpointing_enable()\n",
        "for param in model_qa.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=4,  # Reduced epochs to target 1000 steps\n",
        "    seed=1,\n",
        "    fp16=False,\n",
        "    logging_dir=\"/kaggle/working/logs\",\n",
        "    logging_steps=100,\n",
        "    log_level=\"error\",\n",
        ")\n",
        "\n",
        "\n",
        "# Define the Trainer\n",
        "model_qa_finetuned = Trainer(\n",
        "    model=model_qa,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encodings,\n",
        "    eval_dataset=test_encodings,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:20:44.434265Z",
          "iopub.execute_input": "2024-11-19T17:20:44.434796Z",
          "iopub.status.idle": "2024-11-19T17:20:46.877803Z",
          "shell.execute_reply.started": "2024-11-19T17:20:44.434769Z",
          "shell.execute_reply": "2024-11-19T17:20:46.876874Z"
        },
        "trusted": true,
        "id": "PmrPl8IwaXto",
        "outputId": "280dd1a0-a86e-4aca-c330-491ae3e8d64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa_finetuned.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:20:46.878970Z",
          "iopub.execute_input": "2024-11-19T17:20:46.879274Z",
          "iopub.status.idle": "2024-11-19T17:39:09.058539Z",
          "shell.execute_reply.started": "2024-11-19T17:20:46.879247Z",
          "shell.execute_reply": "2024-11-19T17:39:09.057305Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "ab03dee3241749a787168d582b7020df"
          ]
        },
        "id": "kLTIEcpVaXto",
        "outputId": "17cd53c3-9ddb-401e-9f3e-45e5c6ff4cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112991022221448, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab03dee3241749a787168d582b7020df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241119_172054-oqwecx3m</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/gautham_bot_iit_gn/huggingface/runs/oqwecx3m' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/gautham_bot_iit_gn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/gautham_bot_iit_gn/huggingface' target=\"_blank\">https://wandb.ai/gautham_bot_iit_gn/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/gautham_bot_iit_gn/huggingface/runs/oqwecx3m' target=\"_blank\">https://wandb.ai/gautham_bot_iit_gn/huggingface/runs/oqwecx3m</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 18:10, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.878800</td>\n      <td>2.634984</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.636400</td>\n      <td>0.852152</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.448400</td>\n      <td>0.186643</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.096900</td>\n      <td>0.043052</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.026100</td>\n      <td>0.018259</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.014600</td>\n      <td>0.012186</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.010300</td>\n      <td>0.009604</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.008700</td>\n      <td>0.008250</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.007700</td>\n      <td>0.007564</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.007300</td>\n      <td>0.007349</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=1000, training_loss=0.6135048796534538, metrics={'train_runtime': 1100.8383, 'train_samples_per_second': 3.634, 'train_steps_per_second': 0.908, 'total_flos': 1.195806978048e+16, 'train_loss': 0.6135048796534538, 'epoch': 4.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa_finetuned.save_model(\"/kaggle/working/results\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T18:29:12.664907Z",
          "iopub.execute_input": "2024-11-19T18:29:12.665841Z",
          "iopub.status.idle": "2024-11-19T18:29:26.649830Z",
          "shell.execute_reply.started": "2024-11-19T18:29:12.665803Z",
          "shell.execute_reply": "2024-11-19T18:29:26.648864Z"
        },
        "trusted": true,
        "id": "P39R2tpQaXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "model_qa.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_qa_finetuned = AutoModelForQuestionAnswering.from_pretrained(\"/kaggle/working/results\")\n",
        "model_qa_finetuned.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T18:31:27.466902Z",
          "iopub.execute_input": "2024-11-19T18:31:27.467752Z",
          "iopub.status.idle": "2024-11-19T18:31:41.226546Z",
          "shell.execute_reply.started": "2024-11-19T18:31:27.467719Z",
          "shell.execute_reply": "2024-11-19T18:31:41.225596Z"
        },
        "trusted": true,
        "id": "R3u8rwuuaXtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4:**\n",
        "\n",
        "##Calculate the scores for the following metrics on the test splits for the pre-trained (zero-shot) and fine-tuned models.\n",
        "\n",
        "###**Question-Answering:** squad_v2, F1, METEOR, BLEU, ROUGE, exact-match"
      ],
      "metadata": {
        "id": "glEbcsm5btHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu rouge-score nltk"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T17:41:27.847713Z",
          "iopub.execute_input": "2024-11-19T17:41:27.848472Z",
          "iopub.status.idle": "2024-11-19T17:41:38.418518Z",
          "shell.execute_reply.started": "2024-11-19T17:41:27.848440Z",
          "shell.execute_reply": "2024-11-19T17:41:38.417729Z"
        },
        "trusted": true,
        "id": "iAkCS-XRaXtp",
        "outputId": "6df21c4d-22c2-46bb-b4f9-fc244b2bdab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d359ec4c4add611f1a8df3e83b80426febe4d7ed1a02d939fc383fda726edf71\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: portalocker, sacrebleu, rouge-score\nSuccessfully installed portalocker-3.0.0 rouge-score-0.1.2 sacrebleu-2.4.3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the QA pipeline for the fine-tuned model\n",
        "qa_pipeline_finetuned = pipeline(\"question-answering\", model=model_qa_finetuned, tokenizer=tokenizer)\n",
        "\n",
        "# Pre-trained model (zero-shot evaluation)\n",
        "qa_pipeline_pretrained = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer)\n",
        "\n",
        "def generate_predictions(pipeline, dataset):\n",
        "    predictions, references = [], []\n",
        "    for i in range(300):  # Iterate over the length of the dataset\n",
        "        question = dataset['question'][i]  # Access the question\n",
        "        context = dataset['context'][i]    # Access the context\n",
        "        answer = dataset['answers'][i]['text'][0] if dataset['answers'][i]['text'] else \"\"  # Ground truth answer\n",
        "        references.append(answer)\n",
        "\n",
        "        # Model prediction\n",
        "        result = pipeline({\"question\": question, \"context\": context})\n",
        "        predictions.append(result['answer'])  # Predicted answer\n",
        "        if i%15==0:\n",
        "            print(i)\n",
        "\n",
        "    return predictions, references\n",
        "\n",
        "predictions_finetuned, references = generate_predictions(qa_pipeline_finetuned, test_squad)\n",
        "predictions_pretrained, _ = generate_predictions(qa_pipeline_pretrained, test_squad)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:00:30.787566Z",
          "iopub.execute_input": "2024-11-19T19:00:30.788278Z",
          "iopub.status.idle": "2024-11-19T19:20:56.449215Z",
          "shell.execute_reply.started": "2024-11-19T19:00:30.788243Z",
          "shell.execute_reply": "2024-11-19T19:20:56.448401Z"
        },
        "trusted": true,
        "id": "CFtQY4mpaXtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_match(predictions, references):\n",
        "    matches = [1 if p == r else 0 for p, r in zip(predictions, references)]\n",
        "    return sum(matches) / len(matches) * 100\n",
        "\n",
        "em_finetuned = exact_match(predictions_finetuned, references)\n",
        "em_pretrained = exact_match(predictions_pretrained, references)\n",
        "\n",
        "print(\"Exact Match (Fine-tuned):\", em_finetuned)\n",
        "print(\"Exact Match (Pre-trained):\", em_pretrained)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:21:06.452724Z",
          "iopub.execute_input": "2024-11-19T19:21:06.453620Z",
          "iopub.status.idle": "2024-11-19T19:21:06.461084Z",
          "shell.execute_reply.started": "2024-11-19T19:21:06.453575Z",
          "shell.execute_reply": "2024-11-19T19:21:06.460412Z"
        },
        "trusted": true,
        "id": "G6ql5Rb9aXtp",
        "outputId": "01347315-5c4e-49d8-c720-cccc5ce30535"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Exact Match (Fine-tuned): 6.666666666666667\nExact Match (Pre-trained): 2.3333333333333335\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_metric(predictions, references):\n",
        "    f1_scores = []\n",
        "    for p, r in zip(predictions, references):\n",
        "        pred_tokens = set(p.split())\n",
        "        ref_tokens = set(r.split())\n",
        "        common_tokens = pred_tokens & ref_tokens\n",
        "\n",
        "        if len(common_tokens) == 0:\n",
        "            f1_scores.append(0)\n",
        "        else:\n",
        "            precision = len(common_tokens) / len(pred_tokens)\n",
        "            recall = len(common_tokens) / len(ref_tokens)\n",
        "            f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "    return sum(f1_scores) / len(f1_scores) * 100\n",
        "\n",
        "f1_finetuned = f1_metric(predictions_finetuned, references)\n",
        "f1_pretrained = f1_metric(predictions_pretrained, references)\n",
        "\n",
        "print(\"F1 Score (Fine-tuned):\", f1_finetuned)\n",
        "print(\"F1 Score (Pre-trained):\", f1_pretrained)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:21:11.087325Z",
          "iopub.execute_input": "2024-11-19T19:21:11.087685Z",
          "iopub.status.idle": "2024-11-19T19:21:11.098761Z",
          "shell.execute_reply.started": "2024-11-19T19:21:11.087657Z",
          "shell.execute_reply": "2024-11-19T19:21:11.097691Z"
        },
        "trusted": true,
        "id": "DGrTJTE2aXtq",
        "outputId": "03936750-fc26-4b3a-b9f9-dc5a31f232c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "F1 Score (Fine-tuned): 2.8445865862532527\nF1 Score (Pre-trained): 2.7893975706669205\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "def bleu_metric(predictions, references):\n",
        "    references = [[r] for r in references]  # BLEU expects list of lists\n",
        "    return corpus_bleu(predictions, references).score\n",
        "\n",
        "bleu_finetuned = bleu_metric(predictions_finetuned, references)\n",
        "bleu_pretrained = bleu_metric(predictions_pretrained, references)\n",
        "\n",
        "print(\"BLEU (Fine-tuned):\", bleu_finetuned)\n",
        "print(\"BLEU (Pre-trained):\", bleu_pretrained)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:21:14.904316Z",
          "iopub.execute_input": "2024-11-19T19:21:14.905207Z",
          "iopub.status.idle": "2024-11-19T19:21:14.924012Z",
          "shell.execute_reply.started": "2024-11-19T19:21:14.905164Z",
          "shell.execute_reply": "2024-11-19T19:21:14.923083Z"
        },
        "trusted": true,
        "id": "MhlFTqrMaXtq",
        "outputId": "be2758c3-1df4-48fd-9588-c80b23b41c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "BLEU (Fine-tuned): 0.0\nBLEU (Pre-trained): 21.3643503198117\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocoevalcap\n",
        "from pycocoevalcap.meteor.meteor import Meteor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:26:49.116641Z",
          "iopub.execute_input": "2024-11-19T19:26:49.116985Z",
          "iopub.status.idle": "2024-11-19T19:26:59.871279Z",
          "shell.execute_reply.started": "2024-11-19T19:26:49.116957Z",
          "shell.execute_reply": "2024-11-19T19:26:59.870473Z"
        },
        "trusted": true,
        "id": "EZ0JrRuKaXtq",
        "outputId": "d6217619-9b8a-492e-c4ab-5de0470e45a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pycocotools>=2.0.2 (from pycocoevalcap)\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\nDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.8\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_meteor(pred_texts, label_texts):\n",
        "    meteor = Meteor()\n",
        "    # Wrap predictions and references as dictionaries with keys as 'id' (just use index here for simplicity)\n",
        "    scores = [meteor.compute_score({i: [label]}, {i: [pred]})[0] for i, (label, pred) in enumerate(zip(label_texts, pred_texts))]\n",
        "    score = np.mean(scores)\n",
        "    return score\n",
        "\n",
        "# Compute METEOR scores for both fine-tuned and pre-trained models\n",
        "meteor_finetuned = compute_meteor(predictions_finetuned, references)\n",
        "meteor_pretrained = compute_meteor(predictions_pretrained, references)\n",
        "\n",
        "print(\"METEOR (Fine-tuned):\", meteor_finetuned)\n",
        "print(\"METEOR (Pre-trained):\", meteor_pretrained)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:36:51.814964Z",
          "iopub.execute_input": "2024-11-19T19:36:51.815765Z",
          "iopub.status.idle": "2024-11-19T19:37:10.608385Z",
          "shell.execute_reply.started": "2024-11-19T19:36:51.815732Z",
          "shell.execute_reply": "2024-11-19T19:37:10.607552Z"
        },
        "trusted": true,
        "id": "PIm86WamaXtq",
        "outputId": "86464c52-0cca-4476-e1f8-b6f9e059ea1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "METEOR (Fine-tuned): 0.01787208928954959\nMETEOR (Pre-trained): 0.01965397891457173\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5**\n",
        "\n",
        "##Calculate the number of parameters in the model after fine-tuning. Does it remain the same as the pre-trained model?"
      ],
      "metadata": {
        "id": "P1qmzyW5cZZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "pretrained_params = count_parameters(model_qa.base_model)\n",
        "print(f\"Number of parameters in pre-trained model: {pretrained_params}\")\n",
        "\n",
        "\n",
        "finetuned_params = count_parameters(model_qa_finetuned)\n",
        "print(f\"Number of parameters in fine-tuned model: {finetuned_params}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T19:39:39.270899Z",
          "iopub.execute_input": "2024-11-19T19:39:39.271552Z",
          "iopub.status.idle": "2024-11-19T19:39:39.281310Z",
          "shell.execute_reply.started": "2024-11-19T19:39:39.271519Z",
          "shell.execute_reply": "2024-11-19T19:39:39.280356Z"
        },
        "trusted": true,
        "id": "sdnN9vAkaXtr",
        "outputId": "37739bb8-6985-437b-e8a1-917b62a0fa0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of parameters in pre-trained model: 1235814400\nNumber of parameters in fine-tuned model: 1235818498\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "No, both parameters are not same. The difference in the number of parameters between the pre-trained and fine-tuned models is minimal: 4,098 parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "XmSXtRnFmso6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6:**\n",
        "\n",
        "##Push the fine-tuned model to 🤗."
      ],
      "metadata": {
        "id": "MZgjHUFxcecL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "#model_qa_finetuned.save_pretrained(\"path_to_save_model\")\n",
        "#tokenizer.save_pretrained(\"path_to_save_model\")\n",
        "\n",
        "# Create a repo on Hugging Face (if not already created)\n",
        "create_repo(\"GauthamBot/gautham_fine_tuned\", exist_ok=True)\n",
        "\n",
        "# Upload to Hugging Face\n",
        "upload_folder(\n",
        "    folder_path=\"/kaggle/working/results\",\n",
        "    repo_id=\"GauthamBot/gautham-fine-tune\",\n",
        "    commit_message=\"Fine-tuned model on Squad dataset\"\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T20:00:33.263298Z",
          "iopub.execute_input": "2024-11-19T20:00:33.263679Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "c0f19a7ab30e45588a18e9b23959551e"
          ]
        },
        "id": "JzPyTFl0aXtr",
        "outputId": "59bca1fc-17ae-43aa-c87d-fb9561564e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f19a7ab30e45588a18e9b23959551e"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}
